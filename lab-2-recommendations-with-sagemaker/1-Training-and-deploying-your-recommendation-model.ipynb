{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook outlines how to build a recommendation system using a combination of [SageMaker's Factorization Machines (FM)](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html) and [k-Nearest Neigbor](https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html) built-in algorithms. The main goal is to showcase how to extend FM model to predict top \"X\" recommendations using SageMaker's KNN\" It is based on this blog post:\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/\n",
    "\n",
    "There are four parts to this notebook:\n",
    "\n",
    "1. Training a Factorization Model using the movie lens dataset\n",
    "2. Repackaging FM Model to fit a [k-nearest-neighbors](https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html) Model (KNN)\n",
    "3. Fitting the KNN model\n",
    "4. Deploy a realtime inference endpoint\n",
    "5. Optional -  Batch Transform for predicting top \"X\" items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Training a Factorization Model using the movie lens dataset\n",
    "\n",
    "Julien Simon has written a fantastic blog about how to build a FM model using SageMaker with detailed explanation. Please see the links below for more information. In this part, I utilized his code for the most part to have continutity for performing additional steps.\n",
    "\n",
    "Source - https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "import boto3, io, os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this value to your own bucket name if you want to\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "print(\"Using following s3 bucket: {}\".format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download movie rating data from movie lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf ml-100k/ua.base -o ml-100k/ua.base.shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data\n",
    "\n",
    "First we will load the data into a Pandas dataframe and have a look at the first rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_ratings_train = pd.read_csv('ml-100k/ua.base.shuffled', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_ratings_test = pd.read_csv('ml-100k/ua.test', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users= user_movie_ratings_train['user_id'].max()\n",
    "nb_movies=user_movie_ratings_train['movie_id'].max()\n",
    "nb_features=nb_users+nb_movies\n",
    "nb_ratings_test=len(user_movie_ratings_test.index)\n",
    "nb_ratings_train=len(user_movie_ratings_train.index)\n",
    "print( \" # of users: {}\".format( nb_users))\n",
    "print (\" # of movies: {}\".format(nb_movies))\n",
    "print( \" Training Count: {}\".format(nb_ratings_train))\n",
    "print (\" Test Count:{} \".format(nb_ratings_test))\n",
    "print (\" Features (# of users + # of movies): {}\".format(nb_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data.\n",
    "\n",
    "We will convert the data into a one-hot encoded sparse matrix. Only ratings 4 and above are considered for the model. We will be ignoring ratings 3 and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(df, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    for index, row in df.iterrows():\n",
    "            X[line,row['user_id']-1] = 1\n",
    "            X[line, nb_users+(row['movie_id']-1)] = 1\n",
    "            if int(row['rating']) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line=line+1\n",
    "\n",
    "    Y=np.array(Y).astype('float32')            \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "X_train, Y_train = loadDataset(user_movie_ratings_train, nb_ratings_train, nb_features)\n",
    "X_test, Y_test = loadDataset(user_movie_ratings_test, nb_ratings_test, nb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nb_ratings_train, nb_features)\n",
    "assert Y_train.shape == (nb_ratings_train, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_train-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nb_ratings_test, nb_features)\n",
    "assert Y_test.shape  == (nb_ratings_test, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_test-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Protobuf format for saving to S3\n",
    "\n",
    "Next, we’re going to write the training set and the test set to two protobuf files stored in Amazon S3. Fortunately, we can rely on the write_spmatrix_to_sparse_tensor() utility function. It writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (AKA tensor).\n",
    "\n",
    "Then we commit the buffer to Amazon S3. After this step is complete, we’re done with data preparation, and we can now focus on our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'fm'\n",
    "\n",
    "if bucket.strip() == '':\n",
    "    raise RuntimeError(\"bucket name is empty.\")\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    if d_type == \"sparse\":\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n",
    "    else:\n",
    "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n",
    "        \n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train, bucket, train_prefix, train_key, \"sparse\", Y_train)    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test, bucket, test_prefix, test_key, \"sparse\", Y_test)    \n",
    "  \n",
    "print (\"Training data S3 path: \".format(fm_train_data_path))\n",
    "print (\"Test data S3 path: \".format(fm_test_data_path))\n",
    "print (\"FM model output S3 path: {}\".format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job\n",
    "\n",
    "Let’s start by creating an Estimator based on the Factorization machines container available in our AWS Region. Then, we have to set some FM-specific hyperparameters. hyper parameters are documented in the documentation for the specific built-in algorithm, see [FM hyper parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html)\n",
    "\n",
    "You can play around with the hyper parameters until you are happy with the prediction. For this dataset and hyper parameters configuration, after 100 epochs, test accuracy was around 70% on average and the F1 score (a typical metric for a binary classifier) was around 0.74 (1 indicates a perfect classifier). Not great, but you can fine tune the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type='ml.m5.large'\n",
    "fm = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"factorization-machines\"),\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type=instance_type,\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nb_features,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=100)\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Repackaging Model data to fit a K-Nearest-Neighbor Model\n",
    "\n",
    "The model which we have just trained based on the factorization machines algorithm allows you to predict a score for a pair, such as user, item. The score represents how well the pair matches. However for our use case we want to provide a user as input and receive a list of the top x items that best match the user’s preferences. When the number of items is moderate, you can do this by querying the model for user, item for all possible items. However, this approach doesn’t scale well when the number of items is large. In this scenario, you can use the Amazon SageMaker k-nearest neighbors (k-NN) algorithm to speed up top x prediction tasks.\n",
    "\n",
    "What we will do in this step is extract the latent item and user representations (embeddings) from the trained Factorization model. The assumption is that movies which are close to a user in this latent space are more favourable to the user. We will use this to fit a k-NN model which will allow us to query for the movies with the \"closest distance\".\n",
    "\n",
    "To do this we will first download the trained model, extract the user and item representations and use these to fit a KNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import pickle\n",
    "\n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_full_path = fm.output_path +\"/\"+ fm.latest_training_job.job_name +\"/output/\"+model_file_name\n",
    "print (\"Model Path: {}\".format( model_full_path))\n",
    "\n",
    "#Download FM model \n",
    "os.system(\"aws s3 cp \"+model_full_path+ \" .\")\n",
    "\n",
    "#Extract model file for loading to MXNet\n",
    "os.system(\"tar xzvf \"+model_file_name)\n",
    "os.system(\"unzip -o model_algo-1\")\n",
    "os.system(\"mv symbol.json model-symbol.json\")\n",
    "os.system(\"mv params model-0000.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract model data to create item and user latent matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract model data\n",
    "m = mx.module.Module.load('./model', 0, False, label_names=['out_label'])\n",
    "V = m._arg_params['v'].asnumpy()\n",
    "w = m._arg_params['w1_weight'].asnumpy()\n",
    "b = m._arg_params['w0_weight'].asnumpy()\n",
    "\n",
    "# item latent matrix - concat(V[i], w[i]).  \n",
    "knn_item_matrix = np.concatenate((V[nb_users:], w[nb_users:]), axis=1)\n",
    "knn_train_label = np.arange(1,nb_movies+1)\n",
    "\n",
    "#user latent matrix - concat (V[u], 1) \n",
    "ones = np.ones(nb_users).reshape((nb_users, 1))\n",
    "knn_user_matrix = np.concatenate((V[:nb_users], ones), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save user matrix for later as it will be needed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./user_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(knn_user_matrix, handle)\n",
    "\n",
    "user_matrix_upload_path = fm.output_path +\"/\"+ fm.latest_training_job.job_name + \"/output/user_embeddings.pickle\"\n",
    "os.system(\"aws s3 cp user_embeddings.pickle \"+user_matrix_upload_path)\n",
    "user_matrix_upload_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Building KNN Model\n",
    "\n",
    "In this section, we upload the model input data to S3, create a KNN model and save the same. Saving the model, will display the model in the model section of SageMaker. Also, it will aid in calling batch transform down the line or even deploying it as an end point for real-time inference.\n",
    "\n",
    "This approach uses the default 'index_type' parameter for knn. It is precise but can be slow for large datasets. In such cases, you may want to use a different 'index_type' parameter leading to an approximate, yet fast answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN train features shape = ', knn_item_matrix.shape)\n",
    "knn_prefix = 'knn'\n",
    "knn_output_prefix  = 's3://{}/{}/output'.format(bucket, knn_prefix)\n",
    "knn_train_data_path = writeDatasetToProtobuf(knn_item_matrix, bucket, knn_prefix, train_key, \"dense\", knn_train_label)\n",
    "print('uploaded KNN train data: {}'.format(knn_train_data_path))\n",
    "\n",
    "nb_recommendations = 100\n",
    "\n",
    "# set up the estimator\n",
    "knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "    get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "knn.set_hyperparameters(feature_dim=knn_item_matrix.shape[1], k=nb_recommendations, index_metric=\"INNER_PRODUCT\", predictor_type='classifier', sample_size=200000)\n",
    "fit_input = {'train': knn_train_data_path}\n",
    "knn.fit(fit_input)\n",
    "knn_model_name =  knn.latest_training_job.job_name\n",
    "print (\"created model: {}\".format(knn_model_name))\n",
    "\n",
    "# save the model so that we can reference it in the next step during batch inference\n",
    "sm = boto3.client(service_name='sagemaker')\n",
    "primary_container = {\n",
    "    'Image': knn.image_name,\n",
    "    'ModelDataUrl': knn.model_data,\n",
    "}\n",
    "\n",
    "knn_model = sm.create_model(\n",
    "        ModelName = knn.latest_training_job.job_name,\n",
    "        ExecutionRoleArn = knn.role,\n",
    "        PrimaryContainer = primary_container)\n",
    "print (\"saved the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Deploy a realtime inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnPredictor = knn.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 10\n",
    "\n",
    "def getEmbeddingsForUserAsCSV(user_id):\n",
    "    user= knn_user_matrix[user_id-1][:]\n",
    "    user.shape=(1,user.shape[0])\n",
    "    buf = io.StringIO()\n",
    "    np.savetxt(buf, user, delimiter=',')\n",
    "    return buf.getvalue()\n",
    "\n",
    "user_input = getEmbeddingsForUserAsCSV(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker import RealTimePredictor \n",
    "#knnPredictor = RealTimePredictor('knn-2019-10-24-09-09-02-023')\n",
    "knnPredictor.content_type = 'text/csv'\n",
    "knnPredictor.accept = 'application/jsonlines; verbose=true'\n",
    "result_json = json.loads(knnPredictor.predict(data=user_input))\n",
    "result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Recommended movie Ids for user #{} : {}\".format(user_id, [int(movie_id) for movie_id in result_json['labels']]))\n",
    "print (\"Movie distances for user #{} : {}\".format(user_id,  [round(distance, 4) for distance in result_json['distances']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have now successfully trained and deployed a model which allows us to query a user for a list of movie recommendations. Please execute the following cell and note down below properties as these will be required in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EMBEDDINGS_S3_PATH: {}\".format(user_matrix_upload_path))\n",
    "print(\"SAGEMAKER_ENDPOINT_NAME: {}\".format(knnPredictor.endpoint))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[now jump back into the original Lab Guidebook - Deploying the integration lambda](https://github.com/johanneslanger/recommendations-on-aws-workshop/tree/master/lab-2-recommendations-with-sagemaker#deploying-the-integration-lambda-function)__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
