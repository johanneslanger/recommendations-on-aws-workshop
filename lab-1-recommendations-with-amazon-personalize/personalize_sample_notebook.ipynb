{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Personalize boto3 Client\n",
    "\n",
    "All of the code that we are using in this lab is Python, but any langugage supported by SageMaker could be used.  In this initial piece of code we are loading in the library dependencies that we need for the rest of the lab:\n",
    "\n",
    "- **boto3** - standard Pything CLI library for the AWS SDK\n",
    "- **json** - used to manipulate JSON structures used by our API calls\n",
    "- **numpy** and **pandas** - standard libraries used by Data Scientists everywhere\n",
    "- **time**, **datetime** and **pytz** - used for some time manipulation calls\n",
    "\n",
    "These client handlers will be used throughout the lab, and you will see examples of other client handlers being instantiated for services such as Amazon S3 and IAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "prefix = \"personalize-lab/\"\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Bucket for Our Data\n",
    "\n",
    "Amazon Personalize only supports the uploading of data from an S3 bucket.  Hence, you need to create a new bucket or re-use an existing one.  If you need to create one the you **MUST** name your bucket before running this Code cell by editing the value for *bucket* in the code below.  You need to ensure that the bucket name is globally unique; for this lab we recommend using your name or initials, followed by *-amazon-personalize-lab*, as that is likely to be unique.  Also, you **MUST** ensure that the bucket name contains the work *personalize*, as some IAM permissions later rely on this.\n",
    "\n",
    "If the bucket already exists - such as if you execute this code cell a second time - then it will not create a new bucket, and will not make any changes to the existing bucket.  If this happens unexpectedly then please check your own S3 page in the console to ensure that the bucket is in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Prepare, and Upload Training Data\n",
    "\n",
    "#### Download and Explore the Dataset\n",
    "\n",
    "In this step we download the entire Movie Lens data set zip-file and unzip it - it will go in the same location as the physical notebook *.ipynb* file and the *u.item* file that you downloaded earlier.  We use the **pandas** library to read in the *u.data* file, which contains all of the movie reviews; this file consists of a movie ID, a user ID, a timestamp and a rating of between 1 and 5, and there are 100,000 unique reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -oq ml-100k.zip\n",
    "!rm -f ml-100k.zip\n",
    "data = pd.read_csv('./ml-100k/u.data', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Data\n",
    "\n",
    "We don't actually need all of the review data.  We would like to recommend movies that a user would actually watch based upon what they and others have liked - we don't want to provide a rating between 1 and 5 for every movie in the data set!  Hence, we're going to use **pandas** to drop any reviews that are not > 3.6, and once we've done that also drop the review column - we're left with a subset of the original that contain 4- and 5-star ratings, which is basically indicating movies that the user really liked.  Of course, we could use all of the data, but this would lead to far greater data import and model training time.\n",
    "\n",
    "Additionally, the reviews are quite old - they are from August 1997 to April 1998.  Some of the Amazon Personalize recipies react differently depending upon the age of the interactions - for instance, the _Similar Items_ recipe has several hyperparameters around how to handle 'decaying' interactions.  In order to make this lab easier, and not have to worry about these hyperparameters, we are shifting all of the review timestamps to be from August 2018 up until April 2019.\n",
    "\n",
    "We then write that out to a file named as per that defined two steps previously, and upload it into our S3 bucket.  \n",
    "\n",
    "This is the minimum amount of data that Amazon Personalize needs to train a model on - you need just 1000 rows of user/item/timestamp interactions, but we still have many 10s of thousands of entries left from our original 100,000 review dataset.  This file is known in Amazon Personalize as an **Interactions** data file.  Other data files are usable, such as ones that define additional metadata about the movies (such as year and genre) and another that defines demographic data about the user (such as age, gender and location).  In this lab we do not need them, but this information is available in the Movie Lens data set files that you have downloaded - you can create your own models based upon those at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"DEMO-movie-lens-100k.csv\"              # file in S3 that will hold our model training data\n",
    "data = data[data['RATING'] > 3.6]                  # keep only movies rated 3.6 and above\n",
    "data = data[['USER_ID', 'ITEM_ID', 'TIMESTAMP']]   # select columns that match the columns in the schema below\n",
    "data['TIMESTAMP'] = data['TIMESTAMP'] + 660833618  # make reviews end 1st April 2019 rather than 23rd April 1998\n",
    "data.to_csv(filename, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix + filename).upload_file(filename)\n",
    "\n",
    "# clean up the local files, the csv and the extracted folder\n",
    "!rm -f \"DEMO-movie-lens-100k.csv\"\n",
    "!rm -rf \"ml-100k\"\n",
    "boto3.Session().resource('s3').Object(bucket, prefix + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Here we just create a few similar utility functions to see if we have already created any of the necessary resources, so we don't try and create them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "def find(collectionName, search, nameField, arnField, response):\n",
    "    for item in response[collectionName]:\n",
    "        #print (\"found existing item :: \", item[nameField])\n",
    "        if (item[nameField] == search):\n",
    "            print (\"found our item at arn :: \", item[arnField])\n",
    "            return item[arnField]   \n",
    "    return False\n",
    "\n",
    "def findSchemaArn(search):\n",
    "    nextToken = False\n",
    "    \n",
    "    while (True):\n",
    "        args = {\"maxResults\": 100}\n",
    "        if (nextToken) :\n",
    "            args[\"nextToken\"] = nextToken\n",
    "\n",
    "        response = personalize.list_schemas(**args)\n",
    "        arn = find(\"schemas\", search, \"name\", \"schemaArn\", response)\n",
    "        if (arn):\n",
    "            return arn\n",
    "\n",
    "        if \"nextToken\" in response:\n",
    "            nextToken = response[\"nextToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def findDatasetGroupArn(search):\n",
    "    nextToken = False\n",
    "\n",
    "    while (True):\n",
    "        args = {\"maxResults\": 100}\n",
    "        if (nextToken) :\n",
    "            args[\"nextToken\"] = nextToken\n",
    "\n",
    "        response = personalize.list_dataset_groups(**args)\n",
    "        arn = find(\"datasetGroups\", search, \"name\", \"datasetGroupArn\", response)\n",
    "        if (arn):\n",
    "            return arn\n",
    "\n",
    "        if \"nextToken\" in response:\n",
    "            nextToken = response[\"nextToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def findDatasetArn(datasetGroupArn, search):\n",
    "    nextToken = False\n",
    "    \n",
    "    while(True):\n",
    "        args = {\"maxResults\": 100, \"datasetGroupArn\": datasetGroupArn}\n",
    "        if (nextToken) :\n",
    "            args[\"nextToken\"] = nextToken\n",
    "\n",
    "        response = personalize.list_datasets(**args)\n",
    "        arn = find(\"datasets\", search, \"name\", \"datasetArn\", response)\n",
    "        if (arn):\n",
    "            return arn\n",
    "\n",
    "        if \"nextToken\" in response:\n",
    "            nextToken = response[\"nextToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def findIamRoleArn(search):\n",
    "    nextToken = False\n",
    "    \n",
    "    while(True):\n",
    "        args = {\"MaxItems\": 100}\n",
    "        if (nextToken):\n",
    "            args[\"Marker\"] = nextToken\n",
    "            \n",
    "        response = iam.list_roles(**args)\n",
    "        arn = find(\"Roles\", search, \"RoleName\", \"Arn\", response)\n",
    "        if (arn):\n",
    "            return arn\n",
    "\n",
    "        if \"Marker\" in response and \"IsTruncated\" in response and response[\"IsTruncated\"]:\n",
    "            nextToken = response[\"Marker\"]\n",
    "            print(\"Paginating thru IAM roles\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def findDatasetImportArn(datasetArn, search):\n",
    "    nextToken = False\n",
    "    \n",
    "    while(True):\n",
    "        args = {\"maxResults\": 100, \"datasetArn\": datasetArn}\n",
    "        if (nextToken) :\n",
    "            args[\"nextToken\"] = nextToken\n",
    "\n",
    "        response = personalize.list_dataset_import_jobs(**args)\n",
    "        arn = find(\"datasetImportJobs\", search, \"name\", \"datasetImportJobArn\", response)\n",
    "        if (arn):\n",
    "            return arn\n",
    "\n",
    "        if \"nextToken\" in response:\n",
    "            nextToken = response[\"nextToken\"]\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "Amazon Personalize uses *Schemas* to tell it how to interpret your data files.  This step defines the schema for our Interations file, which consists solely of a `USER_ID`, `ITEM_ID` and `TIMESTAMP`.  Once defined we pass it into Personalize for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "schema_name = \"personalize-lab-recs-schema\"\n",
    "schema_arn = findSchemaArn(schema_name)\n",
    "\n",
    "if (not schema_arn):\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = schema_name,\n",
    "        schema = json.dumps(schema)\n",
    "    )\n",
    "    schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group\n",
    "\n",
    "Now that we have defined a schema, and we have our Interactions data file, we can import the data into Personalize.  But first we have to define a *Dataset Group*, which is essentially a collection of imported data files, trained models and campaigns - each Dataset Group can contain one, and only one, Interaction, Item Metadata and User Demographic file.  When you train a model Personalize will use **all** data files present within its Dataset Group.\n",
    "\n",
    "#### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_name = \"personalize-lab-recs-dataset-group\"\n",
    "dataset_group_arn = findDatasetGroupArn(dataset_group_name)\n",
    "\n",
    "if (not dataset_group_arn):\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = dataset_group_name\n",
    "    )\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status\n",
    "\n",
    "A number of Personalize API calls do take time, hence the calls are asynchronous.  Before we can continue with the next stage we need to poll the status of the `create_dataset_group()` call from the previous code cell - once the Dataset Group is active then we can continue.  **NOTE: this step should not take more than 1-2 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_dataset_group_response[\"datasetGroup\"][\"creationDateTime\"]\n",
    "    print(\"DatasetGroup: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "We now have to create our dataset for the Interactions file.  This step does not actually import any data, rather it creates an internal structure for the data to be imported into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "dataset_name = \"personalize-lab-recs-dataset\"\n",
    "dataset_arn = findDatasetArn(dataset_group_arn, dataset_name)\n",
    "\n",
    "if (not dataset_arn):\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = schema_arn,\n",
    "        name=dataset_name\n",
    "    )\n",
    "\n",
    "    dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare, Create, and Wait for Dataset Import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach policy to S3 bucket\n",
    "\n",
    "Whilst we have created an S3 bucket, and our Interactions data file is sat there waiting to be imported, we have a problem - you may have full access to the bucket via the AWS console or APIs, but the Amazon Personalize service does not.  Hence, you have to create an S3 bucket policy that explicitly grants the service access to the `GetObject` and `ListBucket` commands in S3.  This code step creates such a policy and attaches it to your S3 bucket.\n",
    "\n",
    "Note, any Personalize API calls that need to access you S3 bucket need to be done under the auspices of an IAM role that gives it permission - this step simply allows the service to access the bucket if, and only if, roles with appropriate permissions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy));\n",
    "result = s3.get_bucket_policy(Bucket=bucket)\n",
    "print(json.dumps(result['Policy'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Personalize S3 Role ARN\n",
    "\n",
    "We need to define an IAM role that gives Personalize the ability to access S3 buckets - this is needed as well as the S3 bucket policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_role_name = \"PersonalizeLabRole\"\n",
    "iam_role_arn = findIamRoleArn(iam_role_name)\n",
    "\n",
    "if (not iam_role_arn):\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "              \"Effect\": \"Allow\",\n",
    "              \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "              },\n",
    "              \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"Creating the role now...\");\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = iam_role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "    )\n",
    "\n",
    "    # AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes\n",
    "    # \"personalize\" or \"Personalize\".  \n",
    "    # Since we are using SageMakers default bucket, which therefor has a different name,\n",
    "    # we will be attaching the AmazonS3ReadOnlyAccess policy to this role\n",
    "    policy_arn = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "    iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = policy_arn\n",
    "    )\n",
    "\n",
    "    print(\"Attaching the policy now...\");\n",
    "    time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "    iam_role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    print(iam_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job\n",
    "\n",
    "This pulls together the information that we have on our Dataset, on our S3 bucket, on our Interactions file and a suitable role for Personalize, and then triggers the actual data import process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_import_job_name = \"personalize-lab-recs-dataset-import-job\"\n",
    "dataset_import_job_arn = findDatasetImportArn(dataset_arn, dataset_import_job_name)\n",
    "\n",
    "if (not dataset_import_job_arn):\n",
    "\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = dataset_import_job_name,\n",
    "        datasetArn = dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket, prefix + filename)\n",
    "        },\n",
    "        roleArn = iam_role_arn\n",
    "    )\n",
    "\n",
    "    dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job and Dataset Import Job Run to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of Interactions file import job, as until it is complete we cannot continue.  **Note: this can take anything between 12-25 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - dataset_import_job[\"creationDateTime\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Recipe\n",
    "\n",
    "There are many different algorithm recipes available within Personalize, and this is a list of all supported algorithms.  We are going to select the standard HRNN recipe, which only needs the Interactions file and not the Item metadata or User demographic files.  Amazon Personalize has implemented a hierarchical recurrent neural network (HRNN), which is able to model the changes in user behavior.\n",
    "\n",
    "Temporal modeling is important in recommendation systems because user intent and interests tend to change or drift with time. This drifting is hard to model with traditional approaches. For example, a common approach in factorization machines is to manually come up with a discounting function for distant interactions (distance measured by time). Such manual engineering of weights is human-effort intensive and is prone to inaccuracy.\n",
    "\n",
    "The Amazon Personalize HRNN model, on the other hand, can take ordered user histories and make correct inferences. HRNN uses a gating mechanism to model the discount weights as a learnable function of the items and timestamp. The hierarchical component further improves temporal model efficiency and leads to higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_list = [\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn\",\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn-coldstart\",\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn-metadata\",\n",
    "    \"arn:aws:personalize:::recipe/aws-personalized-ranking\",\n",
    "    \"arn:aws:personalize:::recipe/aws-popularity-count\",\n",
    "    \"arn:aws:personalize:::recipe/aws-sims\"\n",
    "]\n",
    "\n",
    "recipe_arn = recipe_list[0]\n",
    "print(recipe_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Solution\n",
    "\n",
    "With our data imported we can now train our ML solution.  This consists of just a single API to Personalise, where we specify the Dataset to use.\n",
    "\n",
    "#### Create Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"personalize-lab-recs-solution\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = solution_arn\n",
    ")\n",
    "\n",
    "solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of solution creation job, as until it is complete we cannot continue. **Note: this can take anything between 25-50 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_version_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = solution_version_arn\n",
    "    )\n",
    "    status = describe_solution_version_response[\"solutionVersion\"][\"status\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_solution_version_response[\"solutionVersion\"][\"creationDateTime\"]\n",
    "    print(\"SolutionVersion: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Additional Solutions in the Console\n",
    "\n",
    "Whilst you're waiting for this to complete, jump back into the original Lab Guidebook - there we will walk you through creating two additional solutions in parallel using the same dataset; one for Personalized Rankings and one for Item-to-Item Similarities (or SIMS), both of which can be used in the final application.  Once you've begun to create both additional solutions you can come back here and continue.\n",
    "\n",
    "#### Get Metrics of Solution\n",
    "\n",
    "Once the soluition is built you can look up the various metrics that Personalize provides - this allows you to see how well a model has been trained.  If you are re-training models after the acquisition of new data then these metrics can tell you if the models are training equally as well as before, better than before or worse than before, giving you the information that you need in order to decide whether or not to push a new model into Production.  You can also compare results across multiple different algorithm recipes, helping you choose the best performing one for you particular dataset.\n",
    "\n",
    "You can find details on each of the metrics in our [documentation](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(get_solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Campaign\n",
    "\n",
    "A trained model is exactly that - just a model.  In order to use it you need to create an API endpoint, and you do this by creating a *Campaign*.  A Campaign simply provides the endpoint for a specific version of your model, and as such you are able to host endpoint for multiple versions of your models simultaneously, allowing you to do things like A/B testing of new models.\n",
    "\n",
    "At the campaign level we specift the the minimum deployed size of the inference engine in terms of transactions per second - whilst this engine can scale up and down dynamically it will never scale below this level, but please note that pricing for Personalize is heavily based around the number of TPS currently deployed.\n",
    "\n",
    "#### Create campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"personalize-lab-recs-campaign\",\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    minProvisionedTPS = 1\n",
    ")\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of Campaign creation job, as until it is complete we cannot continue.  **Note: this can take anything between 3-15 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_campaign_response[\"campaign\"][\"creationDateTime\"]\n",
    "    print(\"Campaign: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations\n",
    "\n",
    "Finally, we have a deployed Campaign end-point, which hosts a specific version of our trained model - we are now able to make recommendation inference requests against it.  However, the Personalize recommendation calls just return itemID values - it returns no context around the title of the movie,  Hence, we use our pre-loaded version of the *u.item* file that contains the movie titles - we load in the file via **pandas** library calls and pick out a random userID and movieID from our training date.  This info is displayed, along with the name of the movie.\n",
    "\n",
    "#### Select a User and an Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./u.item', sep='\\t', usecols=[0,1], header=None)\n",
    "items.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "user_id, item_id, _ = data.sample().values[0]\n",
    "item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "print(\"USER: {}\".format(user_id))\n",
    "print(\"ITEM: {}\".format(item_title))\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call GetRecommendations\n",
    "\n",
    "The last thing we do is actually make a Personalize recommendations inference call - as you can see from the Code cell this is literally a single line of code with a userID and itemID as input variables (although, strictly speaking, you only need the userID for the datasets that we have).\n",
    "\n",
    "The inference call returns a list of up to 25 itemIDs from the training set - we take that and look up the corresponding movie titles from the *u.item* file and display them; this is far more useful than just a list of ID values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(user_id)\n",
    ")\n",
    "\n",
    "item_list = get_recommendations_response['itemList']\n",
    "title_list = [items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1] for item in item_list]\n",
    "\n",
    "print(\"Recommendations: {}\".format(json.dumps(title_list, indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
